{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc483fef-a071-47f0-bc33-b1442f47daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84340961-2c6b-4559-b3f9-984ae5f78842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_token_chunks(pdf_path, tokenizer, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Converts a PDF file to text chunks based on token count, ensuring that chunks end at logical boundaries.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the input PDF file.\n",
    "    - tokenizer (tiktoken.Encoding): A tiktoken tokenizer instance.\n",
    "    - max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "    - chunks (list): List of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Extract all text from the PDF\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                full_text += text + \"\\n\"  # Add a newline between pages for readability\n",
    "\n",
    "    return chunking(full_text, tokenizer, max_tokens=max_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78033bee-20ca-49fb-b423-379339b9d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_chunks(txt_path, tokenizer, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Converts a TXT file to text chunks based on token count, ensuring chunks end logically.\n",
    "\n",
    "    Parameters:\n",
    "    - txt_path (str): Path to the input txt file.\n",
    "    - tokenizer (tiktoken.Encoding): A tiktoken tokenizer instance.\n",
    "    - max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "    - chunks (list): List of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Read the full text from the TXT file\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as txt:\n",
    "        full_text = txt.read()\n",
    "\n",
    "    return chunking(full_text, tokenizer, max_tokens=max_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09b42931-294c-4bd6-8649-e5817ed971a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(full_text, tokenizer, max_tokens=100):\n",
    "    sentences = full_text.split('\\n')  # Split by newline to preserve sentence boundaries\n",
    "\n",
    "    # Create chunks without breaking sentences\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.encode(sentence)\n",
    "        sentence_length = len(sentence_tokens)\n",
    "\n",
    "        # If adding this sentence exceeds the token limit, finalize the current chunk\n",
    "        if current_tokens + sentence_length > max_tokens:\n",
    "            chunks.append(current_chunk.strip() + '\\n')  # Ensure the chunk ends with a newline\n",
    "            current_chunk = \"\"\n",
    "            current_tokens = 0\n",
    "\n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk += sentence + '\\n'\n",
    "        current_tokens += sentence_length\n",
    "\n",
    "    # Add the last chunk if any text remains\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip() + '\\n')\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16a2d9-af2b-4494-a97f-2c63389c593d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff04d2cf-5625-4170-bb22-550e42e56534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_to_file(chunks, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as file:\n",
    "        for chunk in chunks:\n",
    "            file.write(chunk + \"\\n---\\n\")  # Separate chunks with \"---\" for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "714c297b-d305-41bf-8348-bc052984a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/raw/deadpool/deadpool-2016.pdf\"\n",
    "\n",
    "# Output file for tokenized chunks\n",
    "output_path = \"data/chunks/deadpool/deadpool_script_token_chunks.txt\"\n",
    "\n",
    "# Maximum number of tokens per chunk\n",
    "max_tokens = 512  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4f52d8c-5e2d-4378-a71e-17b70d497532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09cd41cd-ee7e-4275-86a0-2ac082a0b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deadpool_token_chunks = pdf_to_token_chunks(pdf_path, tokenizer, max_tokens=max_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c4ad5f0-ccb8-4700-a49b-28da7684f459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deadpool_token_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9517fa4-4f1b-45d9-8179-a67fef467396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEADPOOL\n",
      "It’s like Christmas Day, Dopinder. Been\n",
      "waiting one thousand eight hundred twenty-\n",
      "two days, three hours...\n",
      "(checks ‘Adventure Time’\n",
      "watch)\n",
      "...and thirty-six minutes for this shit.\n",
      "(CONTINUED)\n",
      "Deadpool Final Shooting Script 11/16/15 2.\n",
      "1 CONTINUED: 1\n",
      "DEADPOOL turns himself RIGHT-SIDE-UP in the front seat. He\n",
      "is YOKED to the gills and ARMED to the teeth. TWIN KATANAS.\n",
      "TWIN DESERT EAGLE .50 CALIBER PISTOLS.\n",
      "Deadpool grabs Dopinder’s OPEN BAG of CORN NUTS. Dopinder\n",
      "isn’t quick enough to stop him. Deadpool gazes out the\n",
      "window onto the city - a teeming, sooty urban sprawl that\n",
      "looks almost... pre-post-apocalyptic.\n",
      "Deadpool turns up his MASK. Dopinder catches a GLIMPSE of\n",
      "the bottom of a SCARRED face. And quickly looks AWAY.\n",
      "Deadpool eats the CORN NUTS. CRUNCH. CRUNCH. Points.\n",
      "DEADPOOL (CONT’D)\n",
      "Nice.\n",
      "Dopinder eyes his DAFFODIL DAYDREAM AIR FRESHENER and takes a\n",
      "deep breath through his nose.\n",
      "DOPINDER\n",
      "Smells good, no?\n",
      "DEADPOOL\n",
      "Not the Daffodil Daydream. The girl.\n",
      "A PICTURE of a young INDIAN WOMAN is taped to the dash.\n",
      "DOPINDER\n",
      "Ah yes. Gita. She is quite lovely. She\n",
      "was supposed to make me a very agreeable\n",
      "wife. Mom and Dad chose her rather\n",
      "excellently. But Gita’s heart has been\n",
      "stolen by my cousin Bandhu. Bandhu is as\n",
      "dishonorable as he is attractive.\n",
      "DEADPOOL\n",
      "Dopinder, I’m starting to think I’m in\n",
      "this cab for a reason.\n",
      "DOPINDER\n",
      "Because you hailed it?\n",
      "DEADPOOL\n",
      "No, my slender brown friend... to give\n",
      "you one crucial piece of advice: Love...\n",
      "is a beautiful thing. When it finds you,\n",
      "the whole world smells like Daffodil\n",
      "Daydream.\n",
      "Deadpool’s own heartbreak is palpable. He takes another\n",
      "deep, cleansing BREATH.\n",
      "(CONTINUED)\n",
      "Deadpool Final Shooting Script 11/16/15 3.\n",
      "1 CONTINUED: (2) 1\n",
      "DEADPOOL (CONT’D)\n",
      "So hold onto love tight. Go at Bandhu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(deadpool_token_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044e3d3-1f2f-4d6d-a435-5edb05cccb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c708607-b40a-496f-87ff-255076c9a894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f740a29-19c8-4082-9dfa-b5d5160d8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = \"data/raw/deadpool/deadpool_2016_subs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e979e1f3-f36d-4ca6-9dbc-d613353a6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "deadpool_token_chunks = text_to_token_chunks(txt_path, tokenizer, max_tokens=max_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ff27dcb-7708-4287-bc9e-ca71a5353820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- (MACHINE-GUN FIRING)\n",
      "- Oh!\n",
      "\n",
      "(GROANS) Four...\n",
      "\n",
      "- (GUN FIRES)\n",
      "- (GROANS)\n",
      "\n",
      "Gotcha.\n",
      "\n",
      "(GRUNTS)\n",
      "\n",
      "Right up main street.\n",
      "\n",
      "Three, two!\n",
      "\n",
      "Stupid! Worth it.\n",
      "\n",
      "(GUNFIRE CONTINUES)\n",
      "\n",
      "(GUN FIRES)\n",
      "\n",
      "(ALL GROANING)\n",
      "\n",
      "(CLINKS)\n",
      "\n",
      "(SNIFFING)\n",
      "\n",
      "Ah!\n",
      "\n",
      "I'm touching myself tonight.\n",
      "\n",
      "Francis!\n",
      "\n",
      "Francis...\n",
      "\n",
      "What the shit-biscuit!\n",
      "\n",
      "Where you at, Francis?\n",
      "\n",
      "(GROANING)\n",
      "\n",
      "(GROANING LOUDLY)\n",
      "\n",
      "You're not Francis.\n",
      "\n",
      "Really?\n",
      "Rolling up the sleeves?\n",
      "\n",
      "(GROANS)\n",
      "\n",
      "WADE:\n",
      "You're probably thinking,\n",
      "\n",
      "\"My boyfriend said\n",
      "this was a superhero movie...\n",
      "\n",
      "\"but that guy in the red suit\n",
      "just turned\n",
      "\n",
      "\"that other guy\n",
      "into a fucking kabab!\"\n",
      "\n",
      "Well, I may be super,\n",
      "but I'm no hero.\n",
      "\n",
      "And yeah, technically,\n",
      "this is a murder.\n",
      "\n",
      "But some of the best love\n",
      "stories start with a murder.\n",
      "\n",
      "And that's exactly\n",
      "what this is, a love story.\n",
      "\n",
      "And to tell it right...\n",
      "\n",
      "I gotta take you back\n",
      "to long before\n",
      "\n",
      "I squeezed this ass\n",
      "into red spandex.\n",
      "\n",
      "MERCHANT: Look, would it help\n",
      "if I slow it down for you?\n",
      "\n",
      "I didn't order the pizza.\n",
      "\n",
      "Is this 7348 Red Ledge Drive?\n",
      "Are you Mr. Merchant?\n",
      "\n",
      "Yeah, the Mr. Merchant who\n",
      "didn't order the fucking pie!\n",
      "\n",
      "Then who placed the call?\n",
      "\n",
      "WADE: I did!\n",
      "\n",
      "(TOILET FLUSHING)\n",
      "\n",
      "Pineapple and olive?\n",
      "\n",
      "Sweet and salty.\n",
      "\n",
      "The fuck are you?\n",
      "\n",
      "The fuck you doing\n",
      "in my crib...\n",
      "\n",
      "Is it burnt crust?\n",
      "\n",
      "I... God,\n",
      "I hope not. Um...\n",
      "\n",
      "Whoa... Man, look,\n",
      "if this is about that poker game.\n",
      "\n",
      "(STAMMERING)\n",
      "I told Howie, I told him that...\n",
      "\n",
      "Okay, uh, look,\n",
      "just take whatever you want.\n",
      "\n",
      "- Thanks.\n",
      "- Sir...\n",
      "\n",
      "...before you do\n",
      "anything to him,\n",
      "\n",
      "do you mind\n",
      "if I a get a big tip?\n",
      "\n",
      "(CHUCKLES)\n",
      "Uh, Jeremy, is it?\n",
      "\n",
      "- Umm-hmm. Yeah.\n",
      "- Wade. Wade Wilson.\n",
      "\n",
      "That is a no go\n",
      "on the tiperoo, Jer.\n",
      "\n",
      "I'm not here for him.\n",
      "\n",
      "I'm here for you.\n",
      "\n",
      "Oh. (CHUCKLES)\n",
      "\n",
      "Okay, wow, dodged\n",
      "a big-time bullet on that one.\n",
      "\n",
      "- Not out of the woods yet.\n",
      "- (GROANS)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(deadpool_token_chunks[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1d34a-bacb-4bd8-9be2-6d0534e489c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
