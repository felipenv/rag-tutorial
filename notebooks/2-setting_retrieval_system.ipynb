{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb9cbcb-9ce5-4b1c-bf7e-7e862b2eb0b1",
   "metadata": {},
   "source": [
    "## Install modules in src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1384e66-ce76-4854-932a-7b40b5f093e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235077a-ba26-4422-870a-3378cd5b7aa6",
   "metadata": {},
   "source": [
    "## Ingest datasets to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351c5b1-0612-4bec-b4a4-768d93f25b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingest import Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809321f4-8bc0-4662-bf92-b016cf2bdd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest = Ingest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9cb2b-069d-4a5a-ae92-08d26b1967da",
   "metadata": {},
   "source": [
    "### Chunking deadpool data\n",
    "- here we have 2 datasets. The subtitles for movie Deadpool & Wolverine as a txt file and the script for deadpool movie as a pdf. We will parse both files and chunk them into text segments up to max_tokens size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c708607-b40a-496f-87ff-255076c9a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = \"../data/raw/deadpool/deadpool_wolverine_2024_subs.txt\"\n",
    "pdf_path = \"../data/raw/deadpool/deadpool-2016.pdf\"\n",
    "max_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f740a29-19c8-4082-9dfa-b5d5160d8840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979e1f3-f36d-4ca6-9dbc-d613353a6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "deadpool_chunks = ingest.pdf_to_chunks(pdf_path, max_tokens=max_tokens)\n",
    "deadpool_wolverine_chunks = ingest.txt_to_chunks(txt_path, max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff27dcb-7708-4287-bc9e-ca71a5353820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk example\n",
    "print(deadpool_wolverine_chunks[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad45e9a-36ad-4cb4-9411-7d43dfe28033",
   "metadata": {},
   "source": [
    "## Add chunks to index\n",
    "- at this stage we would ingest the chunks to a vector database that can store both vectors, text and metadata. For simplicity, we use here a FAISS index, and a metadata file containing the text.\n",
    "- the function below creates a faiss index file and a .npy metadata in the folder specified.\n",
    "- in Production, you can use Milvus, Chroma, Postgres or any supported database to store this information. here is a reference from llamaindex: https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/#vector-store-options--feature-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914dc4fd-428f-44cc-b17d-3b1847090470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ingest.ingest_chunks_to_faiss_with_metadata(chunks=deadpool_chunks, index_name=\"deadpool\", folder=\"../indexes\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceb815-dceb-4999-bcbe-f6ed7d25aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest.ingest_chunks_to_faiss_with_metadata(chunks=deadpool_wolverine_chunks, index_name=\"deadpool\", folder=\"../indexes\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5189277-04b3-4177-b60e-333247d0f6b9",
   "metadata": {},
   "source": [
    "## Test querying the index:\n",
    "- now we will use the retriever function query_index to find similar documents in the collection. the query will be embedded using same embedding model used for the chunks. in the .env you can change for other models. In the example here was used `text-embedding-3-large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ab849-cafe-4702-a43b-d1548380eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b34f5-053b-4e5d-82d4-253a4e956d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b2e0e-398e-49d1-815b-cc32d1f879cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What does Deadpool think about superheroes?\"\n",
    "\n",
    "results = rag.query_index(query_text, index_folder=\"../indexes\", index_name=\"deadpool\", top_k=3)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Match Index: {result['index']}\")\n",
    "    print(f\"Distance: {result['distance']}\")\n",
    "    print(f\"Content: {result['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7633ba-bc7d-438a-8ae1-d38a9bc6fa28",
   "metadata": {},
   "source": [
    "### Chunking Shakespeare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da65d5-fe72-4d99-b75c-53d74410697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = \"../data/raw/shakespeare/hamlet.html\"\n",
    "julius_caesar = \"../data/raw/shakespeare/julius_caesar.html\"\n",
    "othello = \"../data/raw/shakespeare/othello.html\"\n",
    "romeo_juliet = \"../data/raw/shakespeare/romeo_juliet.html\"\n",
    "\n",
    "max_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc9cc9-4279-4441-91cd-4a3fadeb050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_chunks = ingest.html_to_chunks(hamlet, max_tokens=max_tokens)\n",
    "julius_caesar_chunks = ingest.html_to_chunks(julius_caesar, max_tokens=max_tokens)\n",
    "othello_chunks = ingest.html_to_chunks(othello, max_tokens=max_tokens)\n",
    "romeo_juliet_chunks = ingest.html_to_chunks(romeo_juliet, max_tokens=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302f0fb-9ce8-439e-8507-b4713b6d6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(othello_chunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e47b1-1055-44aa-bbdc-f191dd4d2e40",
   "metadata": {},
   "source": [
    "## Add chunks to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d91756-cf9e-4c41-b1fd-4462a2f0837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest.ingest_chunks_to_faiss_with_metadata(chunks=hamlet_chunks, index_name=\"shakespeare\", folder=\"../indexes\", mode=\"append\")\n",
    "ingest.ingest_chunks_to_faiss_with_metadata(chunks=julius_caesar_chunks, index_name=\"shakespeare\", folder=\"../indexes\", mode=\"append\")\n",
    "ingest.ingest_chunks_to_faiss_with_metadata(chunks=othello_chunks, index_name=\"shakespeare\", folder=\"../indexes\", mode=\"append\")\n",
    "ingest.ingest_chunks_to_faiss_with_metadata(chunks=romeo_juliet_chunks, index_name=\"shakespeare\", folder=\"../indexes\", mode=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf4f977-6364-49f9-bbda-d6c848949afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
